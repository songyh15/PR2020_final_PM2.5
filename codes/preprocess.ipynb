{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "%matplotlib inline\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "\n",
    "date_list_all = []\n",
    "date_list_extra = []\n",
    "data_folder = os.walk(r\"../data\")  \n",
    "for path,dir_list,file_list in data_folder:  \n",
    "    for file_name in file_list:  \n",
    "        if file_name[:11] == \"beijing_all\" or file_name[:13] == \"beijing_extra\":\n",
    "            path_now = os.path.join(path, file_name)\n",
    "            \n",
    "            try:\n",
    "                data_now = pd.read_csv(path_now)\n",
    "                a = data_now['date'] \n",
    "                print(path_now)\n",
    "                if file_name[:11] == \"beijing_all\" :\n",
    "                    date_list_all.append(path_now)\n",
    "                elif file_name[:13] == \"beijing_extra\" :\n",
    "                    date_list_extra.append(path_now)\n",
    "            # 跳过异常编码的文件 eg: beijing_all_20141231.csv\n",
    "            # 跳过异常内容文件 eg. beijing_all_20170703.csv -- 08\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "date_list_all.sort()\n",
    "date_list_extra.sort()\n",
    "data_all = pd.read_csv(date_list_all[0])\n",
    "data_extra = pd.read_csv(date_list_extra[0])\n",
    "for i in range(1,len(date_list_all)):\n",
    "    print(date_list_all[i])\n",
    "    data_now = pd.read_csv(date_list_all[i])\n",
    "    data_all = pd.concat([data_all, data_now], axis=0)\n",
    "for i in range(1,len(date_list_extra)):\n",
    "    print(date_list_extra[i])\n",
    "    data_now = pd.read_csv(date_list_extra[i])\n",
    "    data_extra = pd.concat([data_extra, data_now], axis=0)\n",
    "    \n",
    "# 保存数据\n",
    "data_all.to_csv(\"../data_all.csv\")\n",
    "data_extra.to_csv(\"../data_extra.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "data_all = pd.read_csv(\"../data_all.csv\")\n",
    "data_extra = pd.read_csv(\"../data_extra.csv\")\n",
    "\n",
    "# 删去无用的第一列\n",
    "data_all = data_all.iloc[:,1:]\n",
    "data_extra = data_extra.iloc[:,1:]\n",
    "\n",
    "# 删去植物园数据\n",
    "data_all = data_all.drop(columns=['植物园'])\n",
    "data_extra = data_extra.drop(columns=['植物园'])\n",
    "\n",
    "# 删去['PM2.5_24h','PM10_24h','SO2_24h','NO2_24h', 'O3_24h','CO_24h','PM10']\n",
    "data_all = data_all[~(data_all['type']=='PM10_24h')]\n",
    "data_all = data_all[~(data_all['type']=='PM2.5_24h')]\n",
    "data_all = data_all[~(data_all['type']=='PM10')]\n",
    "data_extra = data_extra[~(data_extra['type']=='SO2_24h')]\n",
    "data_extra = data_extra[~(data_extra['type']=='NO2_24h')]\n",
    "data_extra = data_extra[~(data_extra['type']=='CO_24h')]\n",
    "data_extra = data_extra[~(data_extra['type']=='O3_24h')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将all数据和extra数据按时间合并\n",
    "data_mix = pd.DataFrame()\n",
    "for date_now in np.unique(data_all['date'].values): \n",
    "    df_date_all = data_all[data_all['date']==date_now]\n",
    "    df_date_extra = data_extra[data_extra['date']==date_now]\n",
    "    if not df_date_extra.shape[0]:\n",
    "        continue\n",
    "        \n",
    "    for hour_now in np.unique(df_date_all['hour'].values):\n",
    "        df_date_hour_all = df_date_all[df_date_all['hour']==hour_now]\n",
    "        df_date_hour_extra = df_date_extra[df_date_extra['hour']==hour_now]\n",
    "        if df_date_hour_extra.shape[0]==4 and df_date_hour_all.shape[0]==2:       \n",
    "            df_mix_now = pd.concat([df_date_hour_all, df_date_hour_extra], axis=0)\n",
    "            data_mix = pd.concat([data_mix, df_mix_now], axis=0)\n",
    "            print(str(date_now)+' '+str(hour_now))\n",
    "# 合并结果保存为data_mix文件\n",
    "data_mix.to_csv(\"../data_mix.csv\")\n",
    "# 读取文件\n",
    "data_mix = pd.read_csv(\"../data_mix.csv\")\n",
    "data_mix = data_mix.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各地数据分别清洗\n",
    "def data_pass(args):\n",
    "    return data_cleaning(*args)\n",
    "\n",
    "def data_cleaning(data_mix, loc):\n",
    "    print(loc)\n",
    "    data_mix['flag'] = True\n",
    "    for type_now in np.unique(data_mix['type'].values):\n",
    "        list_type_mix = data_mix[data_mix['type']==type_now].index.tolist()\n",
    "        if np.isnan(data_mix.iloc[list_type_mix[0],loc]):\n",
    "            data_mix['flag'].iloc[list_type_mix[0]] = False\n",
    "        if np.isnan(data_mix.iloc[list_type_mix[-1],loc]):\n",
    "            data_mix['flag'].iloc[list_type_mix[-1]] = False\n",
    "        for idx in range(1,len(list_type_mix)-1):\n",
    "            if np.isnan(data_mix.iloc[list_type_mix[idx],loc]):\n",
    "                mid = 0.5*(data_mix.iloc[list_type_mix[idx-1],loc]+data_mix.iloc[list_type_mix[idx+1],loc])\n",
    "                if np.isnan(mid):\n",
    "                    data_mix['flag'].iloc[list_type_mix[idx]] = False\n",
    "                else:\n",
    "                    data_mix.iloc[list_type_mix[idx],loc] = mid\n",
    "                    \n",
    "    for date_now in np.unique(data_mix['date'].values): \n",
    "        list_date_mix = data_mix[data_mix['date']==date_now].index.tolist()\n",
    "        for hour_now in np.unique(data_mix.iloc[list_date_mix,1].values):\n",
    "            list_hour_mix = data_mix.iloc[list_date_mix,1][data_mix.iloc[list_date_mix,1]==hour_now].index.tolist()\n",
    "            if data_mix['flag'].iloc[list_hour_mix].sum() < 6:\n",
    "                data_mix['flag'].iloc[list_hour_mix] = False\n",
    "    \n",
    "    dataset_now = data_mix[data_mix['flag']]    \n",
    "    dataset_now = pd.concat([dataset_now.iloc[:,:3], dataset_now.iloc[:,loc]], axis=1)\n",
    "    \n",
    "    pca=PCA(n_components=1)\n",
    "    data_pca = dataset_now[dataset_now['type']=='PM2.5'].iloc[:,0:3]\n",
    "    data_pca.iloc[:,2] = 'PCA'\n",
    "    typelist = np.unique(dataset_now['type'].values).tolist()\n",
    "    feature_ori = np.zeros([dataset_now[dataset_now['type']=='PM2.5'].shape[0], len(typelist)])\n",
    "    for idx in range(len(typelist)):\n",
    "        feature_ori[:,idx] = dataset_now[dataset_now['type']==typelist[idx]].iloc[:,3].values\n",
    "    feature_pca = pca.fit_transform(feature_ori)\n",
    "    data_pca[dataset_now.columns.values[-1]]=feature_pca\n",
    "    \n",
    "    data_pca_plus = pd.DataFrame()\n",
    "    for idx in range(data_pca.shape[0]):\n",
    "        data_pca_plus = pd.concat([data_pca_plus, data_pca.iloc[idx:idx+1,:]], axis=0)\n",
    "        data_pca_plus = pd.concat([data_pca_plus, dataset_now[dataset_now['type']=='PM2.5'].iloc[idx:idx+1,:]], axis=0)\n",
    "    \n",
    "    return dataset_now,data_pca_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 并行程序，对各地数据分别清洗\n",
    "MultiNum = 17\n",
    "pool = Pool(processes=MultiNum)\n",
    "num_loc = data_mix.shape[1]-3\n",
    "para_data_mix = [data_mix]*num_loc\n",
    "para_loc = list(range(3,data_mix.shape[1]))\n",
    "para_list = list(zip(para_data_mix,para_loc))\n",
    "start = time.time()\n",
    "dataset_loc_list = pool.map(data_pass,para_list)\n",
    "end = time.time()\n",
    "print('training lasted: '+str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存清洗结果\n",
    "for data_clean, data_clean_pca in dataset_loc_list:\n",
    "    filename = '../data_mix_clean/'+data_clean.columns.values.tolist()[-1]+'.csv'\n",
    "    data_clean.to_csv(filename)\n",
    "    filename_pca = '../data_mix_clean_pca/'+data_clean_pca.columns.values.tolist()[-1]+'.csv'\n",
    "    data_clean_pca.to_csv(filename_pca)\n",
    "    \n",
    "# 保存地名\n",
    "np.save('../location.npy',data_mix.columns.values[3:], allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各地数据一起清洗\n",
    "def data_pass(args):\n",
    "    return data_cleaning(*args)\n",
    "\n",
    "def data_cleaning(data_mix, loc):\n",
    "    print(loc)\n",
    "    \n",
    "    data_mix['flag'] = True\n",
    "    for type_now in np.unique(data_mix['type'].values):\n",
    "        list_type_mix = data_mix[data_mix['type']==type_now].index.tolist()\n",
    "        if np.isnan(data_mix.iloc[list_type_mix[0],loc]):\n",
    "            data_mix['flag'].iloc[list_type_mix[0]] = False\n",
    "        if np.isnan(data_mix.iloc[list_type_mix[-1],loc]):\n",
    "            data_mix['flag'].iloc[list_type_mix[-1]] = False\n",
    "        for idx in range(1,len(list_type_mix)-1): \n",
    "            if np.isnan(data_mix.iloc[list_type_mix[idx],loc]):\n",
    "                mid = 0.5*(data_mix.iloc[list_type_mix[idx-1],loc]+data_mix.iloc[list_type_mix[idx+1],loc])\n",
    "                if np.isnan(mid):\n",
    "                    data_mix['flag'].iloc[list_type_mix[idx]] = False\n",
    "                else:\n",
    "                    data_mix.iloc[list_type_mix[idx],loc] = mid\n",
    "                    \n",
    "    return data_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 并行程序，对各地数据一起进行数据清洗\n",
    "MultiNum = 17\n",
    "pool = Pool(processes=MultiNum)\n",
    "\n",
    "num_loc = data_mix.shape[1]-3\n",
    "para_data_mix = [data_mix]*num_loc\n",
    "para_loc = list(range(3,data_mix.shape[1]))\n",
    "para_list = list(zip(para_data_mix,para_loc))\n",
    "\n",
    "start = time.time()\n",
    "datamix_flag_list = pool.map(data_pass,para_list)\n",
    "end = time.time()\n",
    "print('training lasted: '+str(end-start))\n",
    "\n",
    "pool.close() \n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_result = pd.DataFrame()\n",
    "data_mix_filled = data_mix.copy()\n",
    "for loc in para_loc:\n",
    "    data_mix_filled.iloc[:,loc] = datamix_flag_list[loc-3].iloc[:,loc]\n",
    "    flag_result[str(loc)] = datamix_flag_list[loc-3]['flag']\n",
    "flag_result['flag_res'] = False\n",
    "for idx in range(flag_result.shape[0]):\n",
    "    if flag_result.iloc[idx,:].sum() == num_loc:\n",
    "        flag_result['flag_res'].iloc[idx] = True\n",
    "data_mix_filled['flag'] = flag_result['flag_res']\n",
    "\n",
    "for date_now in np.unique(data_mix_filled['date'].values): \n",
    "    list_date_mix = data_mix_filled[data_mix_filled['date']==date_now].index.tolist()\n",
    "    for hour_now in np.unique(data_mix_filled.iloc[list_date_mix,1].values):\n",
    "        list_hour_mix = data_mix_filled.iloc[list_date_mix,1][data_mix_filled.iloc[list_date_mix,1]==hour_now].index.tolist()\n",
    "        if data_mix_filled['flag'].iloc[list_hour_mix].sum() < 6:\n",
    "            data_mix_filled['flag'].iloc[list_hour_mix] = False\n",
    "\n",
    "dataset_now = data_mix_filled[data_mix_filled['flag']]    \n",
    "dataset_now = dataset_now.drop(['flag'],axis=1)\n",
    "\n",
    "# 保存文件\n",
    "filename = 'data_mix_clean_all.csv'\n",
    "dataset_now.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
